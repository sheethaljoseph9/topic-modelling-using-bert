# topic-modelling-using-bert

The goal of this project is to explore and implement topic modeling techniques using BERT (Bidirectional Encoder Representations from Transformers), 
a powerful pre-trained language model. Topic modeling is a widely used natural language processing (NLP) technique that involves uncovering hidden
themes or topics within a collection of text documents. In this project, we will leverage the contextualized representations and deep learning 
capabilities of BERT to perform topic modeling on a given dataset. I used the popular 20 Newsgroups dataset which contains roughly 18000 newsgroups
posts for this project.
The outcome of this project will be a topic modeling system that utilizes BERT to uncover meaningful and relevant topics within a given dataset. 
The system can be applied to various domains and can provide valuable insights for tasks such as document clustering, content recommendation,
and information retrieval.
